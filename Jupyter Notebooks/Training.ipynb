{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load raw data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import cross_val_score,validation_curve,ShuffleSplit,learning_curve\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import svm,tree\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import os\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LetsTraining(X,y, estimator, param=None, param_grid=None, scaler=MinMaxScaler(), n_splits=20):\n",
    "    \"\"\"\n",
    "    this function will first conduct train/test split, then normalize on with 'scaler' on trian set\n",
    "    then cast normalization on test set to avoid data leakage from test set \n",
    "    It will fit the traing data from 'estimator' with parameters of 'param'\n",
    "    Finally, it will print the calculated traning score and test with n_splits\n",
    "    ----------------------------------------------------------------------\n",
    "    X: pd DataFrame \n",
    "    y: pd DataFrame or Series\n",
    "    \n",
    "    estimator: like SVC,LogisticRegression,LinearSVC,etc.\n",
    "    \n",
    "    param: parameters to pass into estimator {'C':1 }\n",
    "    \n",
    "    scaler:Normalization function, Defualt: MinMaxScaler\n",
    "    \n",
    "    n_splits: number of iterations to run and average, Defualt: 20\n",
    "     ----------------------------------------------------------------------\n",
    "     Output:\n",
    "     clf : return the classifier with best test score\n",
    "     cache:  dictionary including 'trainmean','trainstd','testmean','teststd', 'confusion_matrix','scaler'\n",
    "    \"\"\"\n",
    "    scores=[]\n",
    "    trainscores=[]\n",
    "    cnf_matrixes=[]\n",
    "    for i in range(n_splits):\n",
    "        #split Training the test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=i)\n",
    "        \n",
    "        #Feature Normalization\n",
    "        X_train=scaler.fit_transform(X_train)\n",
    "        X_test=scaler.transform(X_test)\n",
    "        \n",
    "        #traing data using estimator\n",
    "        clf=estimator()\n",
    "        if param!=None:\n",
    "            clf=clf.set_params(**param)\n",
    "        \n",
    "        \"\"\"\n",
    "        #grid search to find best parameters\n",
    "        \n",
    "        if param_grid!=None:\n",
    "            #param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty':['l1','l2']}\n",
    "            nfolds=5\n",
    "            grid_search = GridSearchCV(estimator(), param_grid, cv=nfolds,return_train_score=True)\n",
    "            grid_search.fit(X_train,y_train)\n",
    "            print(grid_search.best_params_)\n",
    "            clf=clf.set_params(**grid_search.best_params_)\n",
    "        \"\"\"\n",
    "        \n",
    "        clf.fit(X_train,y_train)\n",
    "        #pass score of test set \n",
    "        scores.append(clf.score(X_test,y_test))\n",
    "        trainscores.append(clf.score(X_train,y_train))\n",
    "        \n",
    "        #calculate confusion matrix\n",
    "        y_pred=clf.predict(X_test)\n",
    "        cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        cnf_matrixes.append(cnf_matrix)\n",
    "        \n",
    "    print(\"The training model is {} with parmameter {}\\n\".format(str(estimator),str(param)))\n",
    "    print(\"The training score is: {:.2f} +- {:.2f}\".format(np.mean(trainscores),np.std(trainscores)))\n",
    "    print(\"The test score is: {:.2f} +- {:.2f}\\n\".format(np.mean(scores),np.std(scores)))\n",
    "    cache={'trainmean':np.mean(trainscores),'trainstd':np.std(trainscores),'testmean':np.mean(scores),'teststd':np.std(scores),'confusion_matrix':cnf_matrixes,'scaler':scaler}\n",
    "    return clf,cache\n",
    "#param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}#X_cor_select\n",
    "LetsTraining(X,y,LogisticRegression,param={'C': 10,'penalty':'l1'})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
